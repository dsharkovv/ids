The required modules for this script can be installed via 

  pip install -r requirements.txt 

The script can be run with

  python main.py
 
in interactive mode or with 

  python main.py --config CONFIG_PATH

where CONFIG_PATH is the path to a JSON configuration file. The Config folder can be used to store those.

In interactive mode one can create (stacked) autoencoders and classifiers. Classifiers can be set up to use the weights, generated by an autoencoder. Instructions on how exactly that works are provided on startup of the program and can be accessed again vie the "help" command. Grid search can only be performed via a configuration file. 

If a JSON configuration file is provided then only grid search can be performed (which can also be used for only one training session if necessary). Please see the example JSON file, which can be found in the JSON_Example folder to get a better idea. 

With "backend" one can specify whether TensorFlow or PyTorch is to be used. The parameter can be one of "tf" or "py". The backend cannot be changed.

The "dataset" can be one of "full", "5c" and "2c", which resambles the full KDD-dataset, the five-class version and the two-class version respectively. If the backend is set to PyTorch, PCA can be performed. To accomodate for that "dataset" can also take the form {name:<string>, pca:<int>}, where name is same as above and pca is an integer specifying the number of principal components. Only a warning will be issued if attribute "pca" is missing and no PCA will be performed.

All commands that have to be performed are specified under a list "commands". Each command is a single grid search. Inside of a command the type of model to be used has to be specified via "model". This can be one of:

'ae' - Grid search on an autoencoder 
'sae' - Grid search on a stacked autoencoder
'cl' - Grid search on a classifier 
'clae' - Grid search on classifer with weights from an autoencoder 
'clsae' - Grid search on classifier with weights from a stacked autoencoder

The next step is specifying the hyperparameters. If one of the first three model options is selected then hyperparameters can be defined with "params". If one of the last two is given - then "params_ae" and "params_cl" need to be given to provide the parameters for the (stacked) autoencoder and the classifier separately. The respective attribute is a dictionary where hyperparameters are the keys which reference lists of values.

The available hyperparameters are listed below: 
  "bs" - (int) defines batch size
  "lr" - (float) defines learning rate
  "af" - (sigmoid or relu) defines activation functions
  "hn" - (int) defines hidden layers
  "kl" - (float) defines KL-divergence weight in autoencoders
  "rho" - (float) defines avg. activation in autoencoders
  "wd" - (float) defines weight decay
  "ne" - (int) defines number of training epochs

Notes: 
with stacked autoencoder and a classifier some commands must consist of lists of values rather than a single value. Value at index i refers to the i-th hidden layer/autoencoder:

"hn" - list of ints in stacked autoencoder and classifier
"af" - list of strings in stacked autoencoder and classifier

stacked autoencoder specific - entry i specifies the value for the i-th autoencoder:

"kl" - list of floats 
"rho" - list of floats
"ne" - list of integers 
"wd" - list of floats

The next field is "options", which in the case of "clae" and "clsae" becomes "options_ae" and "options_cl". Again this is a dictionary. This time it holds the options as keys with their respective values. Available options are: 

  "bn" - (bool) enables batch normalization on all hidden layers
  "do" - (bool) enables dropout on all hidden layers
  "last_only" - (bool) disables training of all but the last layer in a classifier. No effect on autoencoders.
  "es" - (int) enables early stopping. Provided value is the patience i.e. how many epochs to wait before stopping. Must be >= 1. Works with PyTorch only.
  "cs" - (bool) enables check point saver in PyTorch
  "reduce_lr" - Learning rate reduction on plateau. Dictionary of the form {factor: <float>, pateince: <int>}. Factor is the percentage of learning rate to be retained (new_lr = factor * old_lr) and patience as in early stopping. Factor must be between 0.0 and 1.0. Works with PyTorch only.
  "cross_val" - (int) Enables cross-validation on training dataset. The value is an integer that sets k for k-fold cross-validation. The value must be >= 2.

Optionally in each command a "dataset" attribute can be provided, which will change the dataset that will be used from this command until a new one is provided in the same way. The syntax is same as specified above.

Commands will be executed in FIFO order, starting from the top one.

With the PyTorch implementation at the end of each experiment a JSON file is saved, which contains results from the epoch where the best value was observed. The metric that will be tracked with autoencoders is the KL divergence and with classifiers that will be the accuracy. The file is located in: 
 Checkpoints/PyTorch/<date>/grid_search/<model_dir>/results

Through the optional parameter "model_dir" (placed inside of a command), a name for the directory where data will be stored can be given. This directory is located inside:
  Checkpoints/PyTorch/<date>/grid_search/<model_dir>
That is also where checkpoints will be saved if enabled.

If TensorFlow is used the same attribute specifies where data from the current grid search is to be stored. The directory will be located inside 
  Checkpoints/TensorFlow/<date>/grid_search/<model_dir>

If <model_dir> is not given, "gs_i" will be used where i is the i-th grid search, starting from 0. For each configuration in a single grid search a new folder will be created of type <model_name>_i where "model_name" is one of "ae", "sae" and "cl" and i refers to the i-th configuration in the current grid search. Inside each folder a JSON config file is provided that shows information about the hyperparameters used.

In the folder JSON_Example a .json file can be found to illustrate the whole process.

If the programm is opened in interactive mode, data will be stored in:
  Checkpoints/<backend>/<date>/interactive

If no value is provided for a paramter, then a default value is used. Defaults are: 

	batch size: 128
	hidden units: 200
	activation function: sigmoid
	average activation: 0.05
	KL-divergence: 1   
	weight decay: 0.0001
	learning rate: 0.001	
      training epochs: 10

There seems to be a bug with the current TensorFlow version under Windows, where an error is thrown by the checkpoint saver when a path is over 255 characters. The error might be thrown if the utility is placed in a deep folder.

See here for info: https://github.com/una-dinosauria/human-motion-prediction/issues/10
